{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAT 301 Final Group Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2507158009.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    library(AER)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "suppressWarnings(suppressMessages({\n",
    "  library(AER)\n",
    "  library(boot)\n",
    "  library(broom)\n",
    "  library(caTools)\n",
    "  library(caret)\n",
    "  library(cowplot)\n",
    "  library(dplyr)\n",
    "  library(ggplot2)\n",
    "  library(GGally)\n",
    "  library(glmnet)\n",
    "  library(grid)\n",
    "  library(gridExtra)\n",
    "  library(infer)\n",
    "  library(modelr)\n",
    "  library(pROC)\n",
    "  library(repr)\n",
    "  library(doParallel)\n",
    "  library(reshape2)\n",
    "  library(tidyverse)\n",
    "}))\n",
    "\n",
    "\n",
    "cat(\"All necessary packages have been loaded successfully.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Members:\n",
    "- Vivaan Jhaveri (39723044)\n",
    "- Ethan Rajkumar\n",
    "- Michael Wang (32981300)\n",
    "- Ruhani Kaur"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employee turnover is a critical challenge for organizations, with far-reaching consequences for productivity, morale, and financial performance. High turnover rates not only increase recruitment and training costs but also disrupt team dynamics and result in the loss of institutional knowledge. Retaining skilled employees is also essential for maintaining a competitive edge. Understanding the factors that drive employee turnover is key to developing effective retention strategies, making this a vital area of study for both practitioners and researchers.\n",
    "\n",
    "Extensive prior research has identified key predictors of employee turnover, including job satisfaction, organizational commitment, and external job opportunities. Alkahtani (2015) highlights seven critical factors influencing turnover, such as perceived organizational support, supervisor support, and organizational justice. Kanchana and Jayathilaka (2023) demonstrated the significant impact of gender, age, and managerial interaction on turnover, emphasizing the importance of fostering employee engagement. Similarly, Alkaabi et al. (2024) underscored the roles of leadership efficacy, corporate culture, and career advancement opportunities, advocating for strategies such as leadership development programs and flexible work schedules to mitigate turnover risks.\n",
    "\n",
    "Building on this foundation, this report will use the `Employee` dataset, a comprehensive resource containing anonymized data on 4,653 employees. The dataset, sourced from Kaggle, provides information on employee demographics, job characteristics, and work status within the organization.\n",
    "\n",
    "The nine key variables in this dataset include:\n",
    "- `Education`: Categorical variable representing the highest level of education attained by the employee (\"Bachelors\" \"Masters\", \"PhD\").\n",
    "- `JoiningYear`: Numerical variable representing the year the employee joined the company.\n",
    "- `City`: Categorical variable representing the city where the employee is located (\"New Delhi\", \"Bangalore\" \"Pune\").\n",
    "- `PaymentTier`: Categorical variable representing the different salary tiers (1, 2, 3).\n",
    "- `Age`: Numerical variable representing the age of the employee.\n",
    "- `Gender`: Categorical variable representing the gender of the employee (\"Male\", \"Female\").\n",
    "- `EverBenched`: Categorical (binary) variable representing whether the employee has ever been \"benched\" (\"Yes\") or not (\"No\").\n",
    "- `ExperienceInCurrentDomain`: Numerical variable representing years of experience the employee has.\n",
    "- `LeaveOrNot`: Binary response variable representing whether the employee left the company (1) or stayed (0).\n",
    "\n",
    "This dataset provides a framework for identifying the factors that influence employee retention. By analyzing variables such as compensation(`PaymentTier`), benching status(`EverBenched`), and professional experience(`ExperienceInCurrentDomain`) and more, we aim to uncover actionable insights into turnover dynamics.\n",
    "\n",
    "Our research employs logistic regression alongside ridge and lasso regression to predict employee turnover and assess the relative importance of key predictors. Logistic regression offers interpretability and identifies significant predictors, while ridge and lasso regression introduce regularization to address multicollinearity and improve model performance. This approach allows us to address the following questions:\n",
    "1. How can logistic regression, logistic regression with ridge regularization, and logistic regression with lasso regularization be used to predict employee turnover? \n",
    "2. Moreover, how do these methods compare in their ability to identify influential factors, provide model interpretability, and achieve predictive performance?\n",
    "\n",
    "By addressing these questions, our study contributes to the ongoing discourse on employee retention, providing practical strategies for organizations to build more stable and engaged workforces. The findings aim to guide HR professionals in designing data-driven interventions to improve employee satisfaction and reduce turnover."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods and Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Demonstrate that the dataset can be read into R.\n",
    "- Clean and wrangle your data into a tidy format.\n",
    "- Plot the relevant raw data, tailoring your plot to address your question.\n",
    "- Make sure to explore the association of the explanatory variables with the response.\n",
    "- Any summary tables that are relevant to your analysis.\n",
    "- Be sure not to print output that takes up a lot of screen space.\n",
    "- Your EDA must be comprehensive with high quality plots."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset will be split into training and testing subsets to ensure proper model evaluation and reduce overfitting. The training data (`employee_train.csv`) is used for exploratory data analysis, while the test data (`employee_test.csv`) will later be used to evaluate the model's performance. The `LeaveOrNot` variable is converted into a factor to facilitate analysis across different levels. Numerical and categorical variables are separated to ensure proper visualizations and statistical summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "employee_train  <- read.csv(\"data/employee_train.csv\")\n",
    "employee_test <- read.csv(\"data/employee_test.csv\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize `employee_train` by taking a look at the categorical variables and the box plot medians. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "options(warn = -1)\n",
    "# Ensure 'LeaveOrNot' is a factor for proper grouping\n",
    "employee_train <- employee_train %>%\n",
    "  mutate(LeaveOrNot = as.factor(LeaveOrNot))\n",
    "\n",
    "# Calculate variance for each numerical variable by LeaveOrNot\n",
    "numeric_vars <- employee_train %>% select(where(is.numeric), LeaveOrNot)\n",
    "\n",
    "numeric_vars_names <- names(numeric_vars)\n",
    "\n",
    "categorical_vars <- employee_train %>% select(-numeric_vars_names, -LeaveOrNot)\n",
    "categorical_vars$LeaveOrNot <- employee_train$LeaveOrNot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cardinality Plots\n",
    "The following cardinality plots are generated for categorical variables and display the proportion of employees who stayed versus left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cardinality_plots <- lapply(names(categorical_vars)[-ncol(categorical_vars)], function(var) {\n",
    "  ggplot(categorical_vars, aes(x = .data[[var]], fill = LeaveOrNot)) +\n",
    "    geom_bar(position = \"fill\") +\n",
    "    labs(x = var, y = \"Proportion\") +\n",
    "    theme_minimal()\n",
    "})\n",
    "\n",
    "grid.arrange(\n",
    "  grobs = cardinality_plots, \n",
    "  ncol = 2,  # Adjust as needed for layout\n",
    "  top = textGrob(\"Proportion of Categorical Variables by LeaveOrNot\", \n",
    "                 gp = gpar(fontsize = 15, fontface = \"bold\"))  # Customize title size and style here\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Plots for Numerical Variables\n",
    "The following boxplots are generated to explore the distribution and central tendency of numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "box_plots <- lapply(names(numeric_vars)[-ncol(numeric_vars)], function(var) {\n",
    "  ggplot(numeric_vars, aes(x = factor(LeaveOrNot), y = .data[[var]], fill = factor(LeaveOrNot))) +\n",
    "    geom_boxplot() +\n",
    "    theme_minimal()\n",
    "})\n",
    "\n",
    "grid.arrange(\n",
    "  grobs = box_plots,\n",
    "  ncol = 2,  # Adjust layout as needed\n",
    "  top = textGrob(\"Box Plots of Numeric Variables by LeaveOrNot\",\n",
    "                 gp = gpar(fontsize = 15, fontface = \"bold\"))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairwise Plots\n",
    "The following pairwise plots are generated to visualize the relationships and potential multicollinearity between numerical predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 12, repr.plot.height = 9)\n",
    "\n",
    "# Create ggpairs plot for all numeric variables\n",
    "ggpairs(numeric_vars,\n",
    "        aes(color = LeaveOrNot, fill = LeaveOrNot), \n",
    "        title = \"Pairwise Relationships of Numeric Variables by LeaveOrNot\",\n",
    "        upper = list(continuous = wrap(\"cor\", size = 4)), \n",
    "        lower = list(continuous = wrap(\"points\", alpha = 0.3, size = 1)), \n",
    "        diag = list(continuous = wrap(\"densityDiag\"))) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of EDA\n",
    "Correlation values in the plot indicate that `Age` and `ExperienceInCurrentDomain` have negative relationships with `LeaveOrNot`, suggesting that younger employees and those with less domain experience are more likely to leave. Density plots reveal that employees who joined more recently (indicated by peaks in the most recent years) show a higher likelihood of leaving. Scatterplots and histograms help visualize the distribution and clustering within each variable, reinforcing that employees with an earlier `JoiningYear`, higher `Age`, and more `ExperienceInCurrentDomain` tend to stay (red section). The box plots further support these trends by illustrating higher medians for `JoiningYear`, `Age`, and `ExperienceInCurrentDomain` among employees who stayed. \n",
    "\n",
    "After exploring all eight input variables, we conclude that `Education`, `Gender`, `PaymentTier`, `City`, and `EverBenched` may be strong predictors of `LeaveOrNot`, while `Age`, `JoiningYear`, and `ExperienceInCurrentDomain` seem less relevant due to apparent multicollinearity. Since this is merely an exploration of the variables without conducting a proper logistic regression, ridge and lasso regression these conclusions are speculative.\n",
    "\n",
    "Based on these insights, we will proceed to design models for logistic regression, lasso regression, and ridge regression.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods: Plan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe in written English the methods you used to perform your analysis from beginning to end, and narrate the code that does the analysis.\n",
    "- If included, describe the “Feature Selection” process and how and why you choose the covariates of your final model.\n",
    "- Make sure to interpret/explain the results you obtain. It’s not enough to just say, “I fitted a linear model with these covariates, and my R-square is 0.87”.\n",
    "- If inference is the aim of your project, a detailed interpretation of your fitted model is required, as well as a discussion of relevant quantities (e.g., are the coefficients significant? How does the model fit the data)?\n",
    "- A careful model assessment must be conducted.\n",
    "- If prediction is the project's aim, describe the test data used or how it was created.\n",
    "- Ensure your tables and/or figures are labelled with a figure/table number."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Models\n",
    "To predict employee turnover (`LeaveOrNot`), the following models will be employed:\n",
    "1. Logistic Regression:\n",
    "    - A baseline model to predict the binary outcome (0 = stayed, 1 = left).\n",
    "    - Provides interpretability through coefficient estimates, indicating the magnitude and direction of each predictor's effect on turnover.\n",
    "2. Lasso Regression:\n",
    "    - Employs L1 regularization to perform variable selection by shrinking some coefficients to zero.\n",
    "    - Useful for identifying key predictors, especially when multicollinearity is present.\n",
    "3. Ridge Regression:\n",
    "    - Utilizes L2 regularization to mitigate multicollinearity and reduce overfitting.\n",
    "    - Ensures robust model performance by balancing bias and variance.\n",
    "\n",
    "### Potential Limitations\n",
    "- **Linearity Assumption**: Logistic regression assumes a linear relationship between predictors and the log-odds of the outcome. Violations of this assumption can lead to model misspecification.\n",
    "- **Class Imbalance**: The dataset has a higher proportion of employees staying (n=3053) compared to those leaving (n=1600), potentially biasing predictions toward the majority class.\n",
    "- **Overfitting**: Including too many predictors without regularization can lead to overfitting. While ridge and lasso address this, careful tuning of their hyperparameters is essential to avoid under- or over-penalization.\n",
    "- **Optimization Bias**: Ridge and lasso regularization, may lead to optimization bias. This can result in overly optimistic estimates of AUC that do not generalize well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Trained Models\n",
    "### Feature Selection - Logistic, Lasso and Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the predictors and response for training\n",
    "X <- model.matrix(object = LeaveOrNot ~ .,\n",
    "                 data = employee_train)[, -1]\n",
    "y <- as.matrix(employee_train$LeaveOrNot, ncol = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequent code evaluates and compares the performance of logistic regression, ridge regression, and lasso regression using 10-fold cross-validation (CV) on the employee_train dataset, with performance measured by the Area Under the Receiver Operating Characteristic Curve (AUC). Each fold serves as a test set while the remaining data is used for training, ensuring robust performance assessment. Logistic regression provides a baseline model, ridge regression uses L2 regularization to handle multicollinearity and prevent overfitting, and lasso regression applies L1 regularization for feature selection. The average AUC across all folds is calculated for each model to summarize their effectiveness, and the results are stored in a tibble for clear comparison. This approach provides insights into model performance and their ability to generalize to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressWarnings({\n",
    "  set.seed(20010527)\n",
    "  num.folds <- 10\n",
    "  folds <- createFolds(employee_train$LeaveOrNot, k = num.folds)\n",
    "\n",
    "  # Logistic Regression Cross-Validation\n",
    "  logistic_auc <- numeric(num.folds)\n",
    "  for (fold in 1:num.folds) {\n",
    "    train.idx <- setdiff(1:nrow(employee_train), folds[[fold]])\n",
    "    test.idx <- folds[[fold]]\n",
    "    \n",
    "    logistic_model <- glm(LeaveOrNot ~ ., data = employee_train, subset = train.idx, family = \"binomial\")\n",
    "    pred <- predict(logistic_model, newdata = employee_train[test.idx, ], type = \"response\")\n",
    "    \n",
    "    logistic_auc[fold] <- suppressMessages(auc(employee_train$LeaveOrNot[test.idx], pred))\n",
    "  }\n",
    "  logistic_cv_auc <- round(mean(logistic_auc), 7)\n",
    "\n",
    "  # Ridge Regression Cross-Validation\n",
    "  ridge_auc <- numeric(num.folds)\n",
    "  for (fold in 1:num.folds) {\n",
    "    train.idx <- setdiff(1:nrow(employee_train), folds[[fold]])\n",
    "    test.idx <- folds[[fold]]\n",
    "    \n",
    "    ridge_model <- cv.glmnet(X[train.idx, ], y[train.idx], alpha = 0, family = \"binomial\", type.measure = \"auc\")\n",
    "    pred <- predict(ridge_model, newx = X[test.idx, ], s = \"lambda.min\", type = \"response\")\n",
    "    \n",
    "    ridge_auc[fold] <- suppressMessages(auc(y[test.idx], pred))\n",
    "  }\n",
    "  ridge_cv_auc <- round(mean(ridge_auc), 7)\n",
    "\n",
    "  # Lasso Regression Cross-Validation\n",
    "  lasso_auc <- numeric(num.folds)\n",
    "  for (fold in 1:num.folds) {\n",
    "    train.idx <- setdiff(1:nrow(employee_train), folds[[fold]])\n",
    "    test.idx <- folds[[fold]]\n",
    "    \n",
    "    lasso_model <- cv.glmnet(X[train.idx, ], y[train.idx], alpha = 1, family = \"binomial\", type.measure = \"auc\")\n",
    "    pred <- predict(lasso_model, newx = X[test.idx, ], s = \"lambda.min\", type = \"response\")\n",
    "    \n",
    "    lasso_auc[fold] <- suppressMessages(auc(y[test.idx], pred))\n",
    "  }\n",
    "  lasso_cv_auc <- round(mean(lasso_auc), 7)\n",
    "\n",
    "  # Create tibble for results\n",
    "  results <- tibble(\n",
    "    Model = c(\"Logistic Regression\", \"Ridge Regression\", \"Lasso Regression\"),\n",
    "    AUC = c(logistic_cv_auc, ridge_cv_auc, lasso_cv_auc)\n",
    "  )\n",
    "})\n",
    "\n",
    "# Print the tibble\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Results\n",
    "The AUC values on the training set indicate that logistic regression, ridge regression, and lasso regression perform similarly, with AUCs around 0.73. Logistic regression achieves the highest AUC on training data, but this advantage may not hold on a testing set due to potential overfitting. Regularized models like ridge and lasso regression could perform better on unseen data by addressing multicollinearity and reducing overfitting. However, evaluating the models on a testing set is essential to confirm their generalizability and ensure reliable conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying exponeitated coeficents & getting confidence intervals\n",
    "logistic_model_summary <- \n",
    "    tidy(logistic_model, exponentiate = TRUE, conf.int = TRUE)\n",
    "\n",
    "# Filter for significant predictors\n",
    "significant_predictors <- logistic_model_summary %>%\n",
    "  filter(p.value < 0.05)\n",
    "\n",
    "# Display table of significant predictors\n",
    "print(significant_predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the results shows that the factors which have the largest effect on an employee's likelihood of leaving are if they have a Master's degree which increases the log-odds of leaving by 2.13 vs. other education levels, work in Pune which increases the log-odds of leaving by 1.65 vs. other cities, are in Payment tier 2 which increases the log-odds of leaving by 1.84 vs. other payment tiers, were benched before which increases the log-odds of leaving by 1.65 compared to not benched. The other factors seem to have smaller effects such as joining year, whether they are in New Delhi, age, gender, and experience in their current domain. An unexpected result is that all of the significant results determined by p-value are the exact same as the variables selected by the backward selection algorithm, which suggests that the backward selection method may have captured the most impactful predictors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you’ll interpret the results you obtained in the previous section with respect to the main question/goal of your project.\n",
    "\n",
    "Summarize what you found and the implications/impact of your findings.\n",
    "If relevant, discuss whether your results were what you expected to find.\n",
    "Discuss how your model could be improved;\n",
    "Discuss future questions/research this study could lead to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alkaabi, A., Alghizzawi, M., Daoud, M. K., & Ezmigna, I. (2024). Factors affecting employee turnover Intention: an integrative perspective. In Studies in systems, decision and control (pp. 737–748). https://doi.org/10.1007/978-3-031-54383-8_57\n",
    "\n",
    "Alkahtani, A. H. (2015). Investigating Factors that Influence Employees’ Turnover Intention: A Review of Existing Empirical Works. International Journal of Business and Management, 10(12), 152. https://doi.org/10.5539/ijbm.v10n12p152\n",
    "\n",
    "Employee dataset. (2023, September 6). Kaggle. https://www.kaggle.com/datasets/tawfikelmetwally/employee-dataset\n",
    "\n",
    "Kanchana, L., & Jayathilaka, R. (2023). Factors impacting employee turnover intentions among professionals in Sri Lankan startups. PLoS ONE, 18(2), e0281729. https://doi.org/10.1371/journal.pone.0281729"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
